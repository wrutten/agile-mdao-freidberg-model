{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569d931",
   "metadata": {
    "tags": [
     "remove-input",
     "active-ipynb",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from openmdao.utils.notebook_utils import notebook_mode\n",
    "except ImportError:\n",
    "    !python -m pip install openmdao[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e8af7",
   "metadata": {},
   "source": [
    "(sec:openmdao_jax_subpackage)=\n",
    "# Composable functions via `jax` (`openmdao.jax`)\n",
    "\n",
    "Certain functions are useful in a gradient-based optimization context, such as smooth activation functions or differentiable maximum/minimum functions.\n",
    "\n",
    "Rather than provide a component that forces a user to structure their system in a certain way and add more components than necessary, the `openmdao.jax` package is intended to provide a universal source for _composable_ functions that users can use within their own components.\n",
    "\n",
    "Functions in `openmdao.jax` are built using the [jax](https://github.com/google/jax) Python package.\n",
    "This allows users to develop components that use these functions, along with other code written with `jax`, and leverage capabilities of `jax` like automatic differentiation, vectorization, and just-in-time compilation.\n",
    "\n",
    "Many of these functions are focused on providing differentiable forms of strictly non-differentiable functions, such as step responses, absolute value, and minimums or maximums.\n",
    "Near regions where the nominal functions would have invalid derivatives, these functions are smooth but will not perfectly match their non-smooth counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86efbe58",
   "metadata": {},
   "source": [
    "## Available Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201b2ba",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "    .. autofunction:: openmdao.jax.act_tanh\n",
    "        :noindex:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fa1f1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openmdao.jax as omj\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "fig.suptitle('Impact of different parameters on act_tanh')\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "mup001 = omj.act_tanh(x, mu=0.001, z=0.5, a=0, b=1)\n",
    "mup01 = omj.act_tanh(x, mu=0.01, z=0.5, a=0, b=1)\n",
    "mup1 = omj.act_tanh(x, mu=0.1, z=0.5, a=0, b=1)\n",
    "\n",
    "ax[0, 0].plot(x, mup001, label=r'$\\mu$ = 0.001')\n",
    "ax[0, 0].plot(x, mup01, label=r'$\\mu$ = 0.01')\n",
    "ax[0, 0].plot(x, mup1, label=r'$\\mu$ = 0.1')\n",
    "ax[0, 0].legend()\n",
    "ax[0, 0].grid()\n",
    "\n",
    "zp5 = omj.act_tanh(x, mu=0.01, z=0.5, a=0, b=1)\n",
    "zp4 = omj.act_tanh(x, mu=0.01, z=0.4, a=0, b=1)\n",
    "zp6 = omj.act_tanh(x, mu=0.01, z=0.6, a=0, b=1)\n",
    "\n",
    "ax[0, 1].plot(x, zp4, label=r'$z$ = 0.4')\n",
    "ax[0, 1].plot(x, zp5, label=r'$z$ = 0.5')\n",
    "ax[0, 1].plot(x, zp6, label=r'$z$ = 0.6')\n",
    "ax[0, 1].legend()\n",
    "ax[0, 1].grid()\n",
    "\n",
    "a0 = omj.act_tanh(x, mu=0.01, z=0.5, a=0, b=1)\n",
    "ap2 = omj.act_tanh(x, mu=0.01, z=0.5, a=0.2, b=1)\n",
    "ap4 = omj.act_tanh(x, mu=0.01, z=0.5, a=0.4, b=1)\n",
    "\n",
    "ax[1, 0].plot(x, a0, label=r'$a$ = 0.0')\n",
    "ax[1, 0].plot(x, ap2, label=r'$a$ = 0.2')\n",
    "ax[1, 0].plot(x, ap4, label=r'$a$ = 0.4')\n",
    "ax[1, 0].legend()\n",
    "ax[1, 0].grid()\n",
    "\n",
    "bp6 = omj.act_tanh(x, mu=0.01, z=0.5, a=0, b=.6)\n",
    "bp8 = omj.act_tanh(x, mu=0.01, z=0.5, a=0, b=.8)\n",
    "b1 = omj.act_tanh(x, mu=0.01, z=0.5, a=0, b=1)\n",
    "\n",
    "ax[1, 1].plot(x, bp6, label=r'$b$ = 0.6')\n",
    "ax[1, 1].plot(x, bp8, label=r'$b$ = 0.8')\n",
    "ax[1, 1].plot(x, b1, label=r'$b$ = 1.0')\n",
    "ax[1, 1].legend()\n",
    "ax[1, 1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b50cc",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "    .. autofunction:: openmdao.jax.smooth_abs\n",
    "        :noindex:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f5047",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "fig.suptitle('Impact of different parameters on smooth_abs')\n",
    "x = np.linspace(-0.2, 0.2, 1000)\n",
    "\n",
    "mup001 = omj.smooth_abs(x, mu=0.001)\n",
    "mup01 = omj.smooth_abs(x, mu=0.01)\n",
    "mup1 = omj.smooth_abs(x, mu=0.1)\n",
    "\n",
    "ax.plot(x, mup001, label=r'$\\mu$ = 0.001')\n",
    "ax.plot(x, mup01, label=r'$\\mu$ = 0.01')\n",
    "ax.plot(x, mup1, label=r'$\\mu$ = 0.1')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268fc9f7",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "    .. autofunction:: openmdao.jax.smooth_max\n",
    "        :noindex:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cb8c2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "fig.suptitle('Impact of different parameters on smooth_max of sin and cos')\n",
    "x = np.linspace(0.5, 1, 1000)\n",
    "\n",
    "sin = np.sin(x)\n",
    "cos = np.cos(x)\n",
    "\n",
    "mup001 = omj.smooth_max(sin, cos, mu=0.001)\n",
    "mup01 = omj.smooth_max(sin, cos, mu=0.01)\n",
    "mup1 = omj.smooth_max(sin, cos, mu=0.1)\n",
    "\n",
    "ax.plot(x, sin, '--', label=r'$\\sin{x}$')\n",
    "ax.plot(x, cos, '--', label=r'$\\cos{x}$')\n",
    "ax.plot(x, mup01, label=r'$\\mu$ = 0.01')\n",
    "ax.plot(x, mup1, label=r'$\\mu$ = 0.1')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3f93a",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "    .. autofunction:: openmdao.jax.smooth_min\n",
    "        :noindex:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322fd31",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "fig.suptitle('Impact of different parameters on smooth_min of sin and cos')\n",
    "x = np.linspace(0.5, 1, 1000)\n",
    "\n",
    "sin = np.sin(x)\n",
    "cos = np.cos(x)\n",
    "\n",
    "mup001 = omj.smooth_min(sin, cos, mu=0.001)\n",
    "mup01 = omj.smooth_min(sin, cos, mu=0.01)\n",
    "mup1 = omj.smooth_min(sin, cos, mu=0.1)\n",
    "\n",
    "ax.plot(x, sin, '--', label=r'$\\sin{x}$')\n",
    "ax.plot(x, cos, '--', label=r'$\\cos{x}$')\n",
    "ax.plot(x, mup01, label=r'$\\mu$ = 0.01')\n",
    "ax.plot(x, mup1, label=r'$\\mu$ = 0.1')\n",
    "ax.legend(ncol=2)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828da93",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "    .. autofunction:: openmdao.jax.ks_max\n",
    "        :noindex:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24ddab",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from openmdao.jax import ks_max\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "fig.suptitle('Impact of different parameters on ks_max')\n",
    "y = np.random.random(100)\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "rho1 = ks_max(y, rho=10.)\n",
    "rho10 = ks_max(y, rho=100.)\n",
    "rho100 = ks_max(y, rho=1000.)\n",
    "\n",
    "ax.plot(x, y, '.', label='y')\n",
    "ax.plot(x, rho1 * np.ones_like(x), label='ks_max(y, rho=10)')\n",
    "ax.plot(x, rho10 * np.ones_like(x), label='ks_max(y, rho=100)')\n",
    "ax.legend(ncol=1)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea9f69",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from openmdao.jax import ks_min\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "fig.suptitle('Impact of different parameters on ks_min')\n",
    "y = np.random.random(100) + 5\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "rho1 = ks_min(y, rho=10.)\n",
    "rho10 = ks_min(y, rho=100.)\n",
    "rho100 = ks_min(y, rho=1000.)\n",
    "\n",
    "ax.plot(x, y, '.', label='y')\n",
    "ax.plot(x, rho1 * np.ones_like(x), label='ks_min(y, rho=10)')\n",
    "ax.plot(x, rho10 * np.ones_like(x), label='ks_min(y, rho=100)')\n",
    "ax.legend(ncol=1)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed36a21",
   "metadata": {},
   "source": [
    "## Getting derivatives from jax-composed functions\n",
    "\n",
    "If the user writes a function that is composed entirely using jax-based functions (from `jax.numpy`, etc.), then `jax` will in most cases be able to provide derivatives of those functions automatically.\n",
    "\n",
    "The library has several ways of doing this and the best approach will likely depend on the specific use-case at hand.\n",
    "Rather than provide a component to wrap a `jax` function and provide derivatives automatically, consider the following example as a template for how to utilize `jax` in combination with OpenMDAO components.\n",
    "\n",
    "The following component uses the `jax` library's numpy implementation to compute the root-mean-square (rms) of an array of data.  It then passes this data through the `openmdao.jax.act_tanh` activation function.\n",
    "\n",
    "The arguments to `act_tanh` are such that it will return a value of approximately 1.0 if the rms is greater than a threshold value of 0.5, or approximately 0.0 if the rms is less than this value.  This `act_tanh` function is an activation function that smoothly transitions from 0.0 to 1.0 such that it is differentiable. Near the threhold value it will return some value between 0.0 and 1.0.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{rms\\_switch} = \\mathrm{act\\_tanh}\\left(\\sqrt{\\frac{1}{n}\\sum{\\left(x^2\\right)}}\\right)\n",
    "\\end{align}\n",
    "\n",
    "### compute_primal\n",
    "\n",
    "In this particular instance, we declare a method of the component named `compute_primal`.\n",
    "That function name is not special to OpenMDAO and the user could call this function whatever they choose so long as it doesn't interfere with some pre-existing component method name.\n",
    "In addition to the `self` argument, `compute_primal` takes positional arguments to make it compatible with `jax`.\n",
    "We also wrap the method with the `jax.jit` decorator (and use `static_argnums` to inform it that the first argument (`self`) is not relevant to `jax`.\n",
    "\n",
    "### compute\n",
    "Compute in this case is just a matter of passing the values of the inputs to `compute_primal` and populating the outputs with the results.\n",
    "\n",
    "###  compute_partials\n",
    "\n",
    "Computing the partial derivatives across the component is done by passing the inputs to a separate method. Since there are multiple ways of computing the partials with `jax`, this example has four different `_compute_partials_xxx` methods, though only one is used.\n",
    "\n",
    "Again, these method names are not special and are only used in the context of this example.\n",
    "\n",
    "### _compute_partials_jacfwd\n",
    "\n",
    "This uses the `jax.jacfwd` method to compuite the partial derivatives of the calculation with a forward differentiation approach.\n",
    "This approach should be one of the faster options when there are comparatively few inputs versus outputs.\n",
    "\n",
    "Note that because we know that we have many inputs and a single output, the jacobian of this function will be a row vector. Forward differentiation methods are a function of the number of inputs to the function, and thus will most likely be a poor choice if there are many inputs and few outputs.\n",
    "\n",
    "Ultimately it's up to the user to know the sparsity structure when they implement the component and how to populate it correctly. For vector inputs and vector outputs, for instance, the sparsity structure is a diagonal band and the values can be extracted using the `jax.numpy.diagonal` function on the matrix returned by `jax.jacfwd` or `jax.jacrev`\n",
    "\n",
    "### _compute_partials_jacrev\n",
    "\n",
    "This is similar to the previous approach except `jax.jacrev` is used.\n",
    "\n",
    "Reverse differentiation should be faster when the number of outputs of a function is significantly fewer than the number of inputs, such as in reduction operations.\n",
    "\n",
    "### _compute_partials_jvp\n",
    "\n",
    "`jax.jvp` normally needs to be called once for each column/index of an input variable, making a poor choice in this example. Note the code below includes a for loop that sets the _tangent_ corresponding to the sensitivity that is desired, iterating through each element and extracting the corresponding derivative.\n",
    "\n",
    "When the sparsity structure is diagonal, as often happens with vector inputs and vector outputs, then the jacobian can be computed with only a single call to `jvp` where the tangents (seeds in OpenMDAO parlance) are 1.0 for each element of the input.\n",
    "\n",
    "### _compute_partials_vjp\n",
    "\n",
    "`jax.vjp` normally needs to be called once for each row/index of an output variable, making a good choice in this example. Because there is a single scalar output in this case, there is no iteration necessary in this case.\n",
    "\n",
    "The same rule applies for diagonal jacobians. We can specify the _cotangent_ information corresponding to all outputs and evaluate the vector jacobian product a single time, as we do in this case.\n",
    "\n",
    "\n",
    "### Which approach to use?\n",
    "\n",
    "In practice, it's going to be a matter of the user profiling their code to see which of these approaches is fastest.\n",
    "For the example below, some testing indicated that `vjp` was about twice as fast as the other approaches .\n",
    "\n",
    "\n",
    "### `register_jax_component`, `_tree_flatten` and `_tree_unflatten`\n",
    "\n",
    "In order to apply the `jax.jit` \"just-in-time\" compilation to the _methods_ of this class, we need to do a few things.\n",
    "\n",
    "First, the `partial` function from `functools` is used to decorate the method with `jax.jit`, including information that marks the `self` argument as static. This allows the methods to have a signature that `jax.jit` can work with.\n",
    "\n",
    "Because we're referencing `self.options` in the `compute_primal` method, we use the `_tree_flatten` and `_tree_unflatten` methods to let `jax` know how to deal with this data. The usage here assumes that we will never change the value of this option while evaluating our `compute` or `compute_partials` methods.\n",
    "\n",
    "OpenMDAO contains the class decorator `register_jax_component` that automatically handles the registration of a class with `jax` as long as it implements the `_tree_flatten` and `_tree_unflatten` methods.\n",
    "\n",
    "In general, these methods and the registration are only necessary if the component references some attribute of self in its jitted `compute_primal` (or equivalent) methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import openmdao.api as om\n",
    "from openmdao.jax import act_tanh\n",
    "\n",
    "\n",
    "@om.register_jax_component\n",
    "class RootMeanSquareSwitchComp(om.ExplicitComponent):\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.options.declare('vec_size', types=(int,))\n",
    "        self.options.declare('mu', types=(float,), default=0.01)\n",
    "        self.options.declare('threshold', types=(float,), default=0.5)\n",
    "        \n",
    "        # This option is only used for this demonstration.\n",
    "        # The user only needs to implement the partials calculation method\n",
    "        # that makes sense in their applicaiton.\n",
    "        self.options.declare('partials_method',\n",
    "                             values=('jacfwd', 'jacrev', 'jvp', 'jvp_vmap', 'vjp'))\n",
    "    \n",
    "    def setup(self):\n",
    "        n = self.options['vec_size']\n",
    "        self.add_input('x', shape=(n,))\n",
    "        self.add_output('rms', shape=(1,))\n",
    "        self.add_output('rms_switch', shape=(1,))\n",
    "        \n",
    "        # The partials are a dense row in this case (1 row x N inputs)\n",
    "        # There is no need to specify a sparsity pattern.\n",
    "        self.declare_partials(of=['rms', 'rms_switch'], wrt=['x'])\n",
    "\n",
    "        self._partials_method = {'jacfwd': self._compute_partials_jacfwd,\n",
    "                                 'jacrev': self._compute_partials_jacrev,\n",
    "                                 'jvp': self._compute_partials_jvp,\n",
    "                                 'jvp_vmap': self._compute_partials_jvp_vmap,\n",
    "                                 'vjp': self._compute_partials_vjp}\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jacfwd(self, x):\n",
    "        deriv_func = jax.jacfwd(self.compute_primal, argnums=[0])\n",
    "        # Always returns a tuple\n",
    "        drms_dx, dswitch_dx = deriv_func(x)\n",
    "        return drms_dx, dswitch_dx\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jacrev(self, x):\n",
    "        deriv_func = jax.jacrev(self.compute_primal, argnums=[0])\n",
    "        # Always returns a tuple\n",
    "        drms_dx, dswitch_dx = deriv_func(x)\n",
    "        return drms_dx, dswitch_dx\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jvp(self, x):\n",
    "        # Note that JVP is a poor choice here, since the jacobian of each output is a row vector!\n",
    "        \n",
    "        # Multipling the jacobian by a column vector of ones results in a scalar output,\n",
    "        # and we are unable to identify the elements in the individual columns of the\n",
    "        # jacobian.\n",
    "        \n",
    "        # Instead, we have to set one element of the tangent to 1 while the rest are zero and\n",
    "        # evaluate the jvp and extract the value in the corresponding column of the jacobian.\n",
    "        # We have no choice but to do this one element at a time, so if the size of x is large,\n",
    "        # this gets prohibitively expensive.\n",
    "        drms_dx = jnp.zeros_like(x)\n",
    "        dswitch_dx = jnp.zeros_like(x)\n",
    "        tangents_x = jnp.zeros_like(x)\n",
    " \n",
    "        for i in range(len(x)):\n",
    "            tangents_x = tangents_x.at[i].set(1.)\n",
    "            # jvp always returns the primal and the jvp\n",
    "            (rms, switch), (drms, dswitch) = jax.jvp(self.compute_primal,\n",
    "                                                     primals=(x,),\n",
    "                                                     tangents=(tangents_x,))\n",
    "            drms_dx = drms_dx.at[i].set(drms)\n",
    "            dswitch_dx = dswitch_dx.at[i].set(dswitch)\n",
    "            tangents_x = tangents_x.at[i].set(0.)\n",
    "\n",
    "        return drms_dx, dswitch_dx\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jvp_vmap(self, x):\n",
    "        # This is a somewhat faster way that computes the partials via JVP with\n",
    "        # by vectorizing the process using jax.vmap.\n",
    "        tangents_x = jnp.eye(len(x))\n",
    "        pushfwd = partial(jax.jvp, self.compute_primal, (x,))\n",
    "        return jax.vmap(pushfwd, out_axes=(None, 0))((tangents_x,))[1]\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_vjp(self, x):\n",
    "        # VJP is a good choice here since the jacbian matrix is a row vector.\n",
    "        # We can compute the jacobian with a single call of the VJP function.\n",
    "   \n",
    "        # vjp always returns the primal and the vjp\n",
    "        primal, vjp_fun = jax.vjp(self.compute_primal, x)\n",
    "        \n",
    "        # Get the partials drms_dx\n",
    "        cotangents = (jnp.ones_like(primal[0]), jnp.zeros_like(primal[0]))\n",
    "        drms_dx = vjp_fun(cotangents)\n",
    "        \n",
    "        # Get the partials drmsswitch_dx\n",
    "        cotangents = (jnp.zeros_like(primal[0]), jnp.ones_like(primal[0]))\n",
    "        dswitch_dx = vjp_fun(cotangents)\n",
    "        \n",
    "        return drms_dx, dswitch_dx\n",
    " \n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def compute_primal(self, x):\n",
    "        n = self.options['vec_size']\n",
    "        mu = self.options['mu']\n",
    "        z = self.options['threshold']\n",
    "        rms = jnp.sqrt(jnp.sum(x**2) / n)\n",
    "        return rms, act_tanh(rms, mu, z, 0.0, 1.0)\n",
    "    \n",
    "    def compute(self, inputs, outputs):  \n",
    "        outputs['rms'], outputs['rms_switch'] = self.compute_primal(*inputs.values())\n",
    "        \n",
    "    def compute_partials(self, inputs, partials):\n",
    "        f_partials = self._partials_method[self.options['partials_method']]\n",
    "        drms_dx, dswitch_dx = f_partials(*inputs.values())\n",
    "        partials['rms', 'x'] = drms_dx\n",
    "        partials['rms_switch', 'x'] = dswitch_dx\n",
    "\n",
    "    def _tree_flatten(self):\n",
    "        \"\"\"\n",
    "        Per the jax documentation, these are the attributes\n",
    "        of this class that we need to reference in the jax jitted\n",
    "        methods of the class.\n",
    "        There are no dynamic values or arrays, only self.options is used.\n",
    "        Note that we do not change the options during the evaluation of\n",
    "        these methods.\n",
    "        \"\"\"\n",
    "        children = tuple()  # arrays / dynamic values\n",
    "        aux_data = {'options': self.options}  # static values\n",
    "        return (children, aux_data)\n",
    "\n",
    "    @classmethod\n",
    "    def _tree_unflatten(cls, aux_data, children):\n",
    "        \"\"\"\n",
    "        Per the jax documentation, this method is needed by jax.jit since\n",
    "        we are referencing attributes of the class (self.options) in our \n",
    "        jitted methods.\n",
    "        \"\"\"\n",
    "        return cls(*children, **aux_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5c270",
   "metadata": {},
   "source": [
    "In the following use case, we use the `vjp` method to compute the partials. You can experiment with the other methods available for this component (`jacfwd`, `jacrev`, and `jvp` to compare timings.\n",
    "\n",
    "Note that testing the derivatives will get much more expensive as N grows due to the number of elements in `x` that need to be perturbed during the complex-step or finite-difference processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19c963",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for deriv_method in ['jacfwd', 'jacrev', 'jvp', 'jvp_vmap', 'vjp']:\n",
    "    N = 100\n",
    "    np.random.seed(16)\n",
    "\n",
    "    p = om.Problem()\n",
    "    p.model.add_subsystem('counter',\n",
    "                          RootMeanSquareSwitchComp(vec_size=N, partials_method=deriv_method),\n",
    "                          promotes_inputs=['x'], promotes_outputs=['rms', 'rms_switch'])\n",
    "\n",
    "\n",
    "    p.setup(force_alloc_complex=True)\n",
    "    p.set_val('x', np.random.random(N))\n",
    "    p.run_model()\n",
    "\n",
    "    print('Derivative method: {deriv_method}')\n",
    "    print('rms = ', p.get_val('rms'))\n",
    "    print('rms_switch = ', p.get_val('rms_switch'))\n",
    "\n",
    "    print('\\nchecking partials')\n",
    "    with np.printoptions(linewidth=1024):\n",
    "        cpd = p.check_partials(method='cs', compact_print=True);\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef56f31",
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from openmdao.utils.assert_utils import assert_check_partials\n",
    "\n",
    "assert_check_partials(cpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8aae02",
   "metadata": {},
   "source": [
    "## Example 2: A component with vector inputs and outputs\n",
    "\n",
    "A common pattern is to have a vectorized input and a corresponding vectorized output.\n",
    "For a simple vectorized calculation this will typically result in a diagonal jacobian, where the\n",
    "n-th element of the input only impacts the n-th element of the output.\n",
    "\n",
    "For this case, a single evaluation of either `jax.jvp` or `jax.vjp` works to compute the derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4be7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@om.register_jax_component\n",
    "class SinCosComp(om.ExplicitComponent):\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.options.declare('vec_size', types=(int,))\n",
    "        \n",
    "        # This option is only used for this demonstration.\n",
    "        # The user only needs to implement the partials calculation method\n",
    "        # that makes sense in their applicaiton.\n",
    "        self.options.declare('partials_method',\n",
    "                             values=('jacfwd', 'jacrev', 'jvp', 'vjp'))\n",
    "    \n",
    "    def setup(self):\n",
    "        n = self.options['vec_size']\n",
    "        self.add_input('x', shape=(n,))\n",
    "        self.add_output('sin_cos_x', shape=(n,))\n",
    "        \n",
    "        # The partials are a dense row in this case (1 row x N inputs)\n",
    "        # There is no need to specify a sparsity pattern.\n",
    "        ar = np.arange(n, dtype=int)\n",
    "        self.declare_partials(of='sin_cos_x', wrt='x', rows=ar, cols=ar)\n",
    "\n",
    "        self._partials_method = {'jacfwd': self._compute_partials_jacfwd,\n",
    "                                 'jacrev': self._compute_partials_jacrev,\n",
    "                                 'jvp': self._compute_partials_jvp,\n",
    "                                 'vjp': self._compute_partials_vjp}\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jacfwd(self, x):\n",
    "        deriv_func = jax.jacfwd(self.compute_primal, argnums=[0], holomorphic=self.under_complex_step)\n",
    "        # Here we make sure we extract the diagonal of the computed jacobian, since we\n",
    "        # know it will have the only non-zero values.\n",
    "        return jnp.diagonal(deriv_func(x)[0])\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jacrev(self, x):\n",
    "        deriv_func = jax.jacrev(self.compute_primal, argnums=[0], holomorphic=self.under_complex_step)\n",
    "        # Here we make sure we extract the diagonal of the computed jacobian, since we\n",
    "        # know it will have the only non-zero values.\n",
    "        return jnp.diagonal(deriv_func(x)[0])\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jvp(self, x):\n",
    "        # JVP is a good choice in this situation.\n",
    "        \n",
    "        # Because the jacobian is diagonal, the product of the jacobian times a\n",
    "        # column vector of ones gives a column vector of the diagonal jacobian values.\n",
    "        tangents_x = jnp.ones_like(x)\n",
    " \n",
    "        sin_cos_x, jvp = jax.jvp(self.compute_primal,\n",
    "                                 primals=(x,),\n",
    "                                 tangents=(tangents_x,))\n",
    "\n",
    "        return jvp\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_vjp(self, x):\n",
    "        # VJP is a good choice here for the same reason that JVP is a good choice.\n",
    "   \n",
    "        # vjp always returns the primal and the vjp\n",
    "        primal, vjp_fun = jax.vjp(self.compute_primal, x)\n",
    "        \n",
    "        # Get the partials drms_dx\n",
    "        cotangents = jnp.ones_like(primal)\n",
    "        return vjp_fun(cotangents)[0]\n",
    "        \n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def compute_primal(self, x):\n",
    "        return jnp.sin(jnp.cos(x))\n",
    "    \n",
    "    def compute(self, inputs, outputs):  \n",
    "        outputs['sin_cos_x'] = self.compute_primal(*inputs.values())\n",
    "        \n",
    "    def compute_partials(self, inputs, partials):\n",
    "        f_partials = self._partials_method[self.options['partials_method']]\n",
    "        # Since the partials are sparse and stored in a flat array, ravel\n",
    "        # the resulting derivative jacobian.\n",
    "        partials['sin_cos_x', 'x'] = f_partials(*inputs.values()).ravel()\n",
    "\n",
    "    def _tree_flatten(self):\n",
    "        \"\"\"\n",
    "        Per the jax documentation, these are the attributes\n",
    "        of this class that we need to reference in the jax jitted\n",
    "        methods of the class.\n",
    "        There are no dynamic values or arrays, only self.options is used.\n",
    "        Note that we do not change the options during the evaluation of\n",
    "        these methods.\n",
    "        \"\"\"\n",
    "        children = ()  # arrays / dynamic values\n",
    "        aux_data = {'options': self.options}  # static values\n",
    "        return (children, aux_data)\n",
    "\n",
    "    @classmethod\n",
    "    def _tree_unflatten(cls, aux_data, children):\n",
    "        \"\"\"\n",
    "        Per the jax documentation, this method is needed by jax.jit since\n",
    "        we are referencing attributes of the class (self.options) in our \n",
    "        jitted methods.\n",
    "        \"\"\"\n",
    "        return cls(*children, **aux_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "np.random.seed(16)\n",
    "\n",
    "p = om.Problem()\n",
    "p.model.add_subsystem('scx',\n",
    "                      SinCosComp(vec_size=N, partials_method='jacfwd'),\n",
    "                      promotes_inputs=['x'], promotes_outputs=['sin_cos_x'])\n",
    "\n",
    "\n",
    "p.setup(force_alloc_complex=True)\n",
    "p.set_val('x', np.random.random(N))\n",
    "p.run_model()\n",
    "\n",
    "print('sin(cos(x)) = ', p.get_val('sin_cos_x'))\n",
    "\n",
    "print('\\nchecking partials')\n",
    "with np.printoptions(linewidth=1024):\n",
    "    cpd = p.check_partials(method='fd', compact_print=False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a361b",
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "assert_check_partials(cpd, atol=1.0E-5, rtol=1.0E-5)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
